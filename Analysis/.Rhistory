set.seed(42)      # set seed for reproducibility
# ----- Session set-up -----
# Load necessary libraries
library(caret)
library(ranger)
library(coin)
library(rsample)
library(doParallel)
library(forcats)
library(glue)
library(gtable)
library(furrr)
library(tidyverse)
# Export plots and data?
export = FALSE
# Parallize? 32 cores if on threadripper - 8 if local
parallel = TRUE
# Model tuning or predefined parameters?
tuning = FALSE
# Check which device is running
node_name <- Sys.info()["nodename"]
# Set file paths conditionally
path_out <- ifelse(node_name == "threadeast", "/home/merlin/traits_output", # ADJUST PATH PLEASE
"/Users/serpent/Documents/MSc/Thesis/Code")
if (parallel) { # set cluster
num_cores <-  ifelse(node_name == "threadeast", 32, 8)
cl <- makeCluster(num_cores)
registerDoParallel(cl, cores = num_cores)
getDoParWorkers()
}
## ---------- Fit Random Forest Models and Compute Shapley Values ----------
# Read input data
data <- read_csv("/Users/serpent/Desktop/traits_merlin.csv")   # ADD PATH TO 'traits_merlin.csv' PLEASE
data <- sample_n(data, 100)
# Split into training and test sets
split <- initial_split(data, prop = 0.8)
# Extract training and testing datasets
train_data <- training(split)
test_data  <- testing(split)
# Define traits
traits <- c("wood_density", "bark_thickness", "conduit_diam", "leaf_n",
"specific_leaf_area", "seed_dry_mass", "shade_tolerance", "height")
# Define covariates
covariates <- c("standage", "temp_pc", "soil_pc", "rain_pc", "elevation", "soil_ph",
"biome_boreal_forests_or_taiga", "biome_flooded_grasslands",
"biome_mediterranean_woodlands", "biome_temperate_broadleaf_forests",
"biome_temperate_conifer_forests", "biome_temperate_grasslands",
"biome_tundra", "biome_xeric_shrublands")
# Define function to perform cross-validation and find the best hyperparameters
tune_rf_model <- function(trait, data, covariates, hyper_grid, num_threads = 1) {
# Define model formula
formula <- as.formula(paste(trait, "~", paste(covariates, collapse = " + ")))
# Perform tuning
prediction_error <- foreach(
num.trees = hyper_grid$num.trees,
mtry = hyper_grid$mtry,
min.node.size = hyper_grid$min.node.size,
.combine = 'c',
.packages = "ranger"
) %dopar% {
# Fit models
mod <- ranger::ranger(
formula = formula,
data = data,
dependent.variable.name = trait,
num.trees = num.trees,
mtry = mtry,
min.node.size = min.node.size,
num.threads = num_threads)
# Return prediction error
return(mod$prediction.error)
}
# Plot prediction errors
error_plot <- hyper_grid %>%
mutate(prediction_error = prediction_error) %>%
ggplot(aes(x = mtry, y = as.factor(min.node.size), fill = prediction_error)) +
facet_wrap(~ num.trees) +
geom_tile() +
scale_y_discrete(breaks = c(1, 10, 20)) +
scale_fill_viridis_c() +
ylab("min.node.size") +
ggtitle(trait) +
theme(text = element_text(family = "sans", size = 6),
legend.key.height = unit(3, "mm"),
legend.key.width = unit(3, "mm"),
plot.title = element_text(face = "bold", size = 6))
# Get best set
best_hyperparameters <- hyper_grid %>%
mutate(trait = trait) %>%
arrange(prediction_error) %>%
slice(1)
return(list(error_plot = error_plot, best_hyperparameters = best_hyperparameters))
}
# Define function to fit a random forest with best hyperparameters per trait
fit_rf_model <- function(trait, data, covariates, hyper_parameters, num_threads = 1) {
# Define model formula for each trait
formula <- as.formula(paste(trait, "~", paste(covariates, collapse = " + ")))
# Get trait-specific hyperparameters
hyper_grid <- hyper_parameters[hyper_parameters$trait == trait, ]
# Fit model
trait_mod <- ranger::ranger(
formula = formula,
data = data,
num.trees = hyper_grid$num.trees[1],
mtry = hyper_grid$mtry[1],
min.node.size = hyper_grid$min.node.size[1],
num.threads = num_threads)
# Get performance metrics
performance <- data.frame(
trait = trait,
mtry = trait_mod$mtry,
num_trees = trait_mod$num.trees,
min_node_size = trait_mod$min.node.size,
rsq = trait_mod$r.squared,
pred_error = trait_mod$prediction.error)
# Return model and performance metrics
return(list(trait_mod = trait_mod, performance = performance))
}
# Tuning if TRUE, else, fit model with predefined parameters
if (tuning) {
hyper_grid <- expand.grid(
num.trees = c(500, 1000, 1500),
mtry = 2:4,
min.node.size = c(1, 10, 20))
# Get tuning results
tuning_result <- map(traits, ~ tune_rf_model(.x, train_data, covariates, hyper_grid, num_threads = ifelse(node_name == "threadeast", 4, 1)))
names(tuning_result) <- traits
hyper_grid <- map(tuning_result, "best_hyperparameters") %>% bind_rows()
tuning_error_plot <- plot_grid(plotlist = map(tuning_result, "error_plot"), ncol = 2, nrow = 4)
if (export) {
ggsave(paste0(path_out, "/output/plots/supplementary/s5.png"),
plot = tuning_error_plot,
bg = "white",
width = 200,
height = 130,
units = "mm",
dpi = 600)
}
rm(tuning_error_plot, tuning_result)
} else {
# Use a default hyper_grid with known best values
hyper_grid <- tibble(
trait = traits,
num.trees = rep(500, length(traits)),
mtry = rep(4, length(traits)),
min.node.size = rep(1, length(traits)))
}
# Fit models in parallel using foreach
best_models <- foreach(trait = traits, .packages = c('ranger', 'dplyr')) %dopar% {
fit_rf_model(trait, data, covariates, hyper_grid, num_threads = ifelse(node_name == "threadeast", 4, 1))
}
# Extract performance metrics and models
names(best_models) <- traits
performance_metrics <- lapply(best_models, `[[`, "performance") %>% bind_rows()
best_models <- map(best_models, "trait_mod")
## --- Bootstrap top models and calculate shapley values ---
# Prediction function for fastshap
predict_fn <- function(object, newdata) {
predict(object, data = newdata)$prediction
}
# Define number of bootstrap iterations
num_bootstraps <- 100
# Function to train models and compute SHAP values per bootstrap iteration
bootstrap_shap <- function(data, traits, covariates, hyper_grid, num_threads = 4, boot_id) {
# Sample with replacement
boot_split <- initial_split(data, prop = 0.8, strata = NULL)
train_data <- training(boot_split)
test_data <- testing(boot_split)
# Fit models in parallel using foreach
best_models <- foreach(trait = traits, .packages = c('ranger', 'dplyr')) %dopar% {
fit_rf_model(trait, train_data, covariates, hyper_grid, num_threads)
}
# Extract models
best_models <- map(best_models, "trait_mod")
# Compute SHAP values for each trait
shap_values_list <- foreach(i = seq_along(best_models), .combine = 'rbind', .packages = c('fastshap', 'dplyr', 'tidyverse')) %dopar% {
model <- best_models[[i]]
trait <- traits[i]
shap_values <- fastshap::explain(model,
X = test_data %>% select(all_of(covariates)) %>% as.matrix(),
pred_wrapper = predict_fn,
nsim = 10,
parallel = TRUE)
shap_values <- as.data.frame(shap_values)
shap_values$trait <- trait
shap_values$bootstrap_id <- boot_id  # Track bootstrap iteration
# Reshape for external processing
shap_values %>%
pivot_longer(-c(trait, bootstrap_id), names_to = "variable", values_to = "shap_value")
}
return(shap_values_list)
}
bootstrap_results <- foreach(i = 1:num_bootstraps, .combine = 'rbind', .packages = c('dplyr', 'ranger', 'fastshap', 'tidyverse', 'foreach', 'rsample')) %dopar% {
bootstrap_shap(data, traits, covariates, hyper_grid, num_threads = 4, boot_id = i)
}
# Compute weighted SHAP importance per category
shap_summary_boot <- bootstrap_results %>%
mutate(category = case_when(
variable == "standage" ~ "Temporal Succession",
TRUE ~ "Environmental Filtering"
)) %>%
group_by(trait, category, variable, bootstrap_id) %>%
summarise(total_shap = sum(abs(shap_value)), .groups = "drop") %>%
group_by(trait, category, bootstrap_id) %>%
mutate(weight = total_shap / sum(total_shap)) %>%  # Weighting each predictor relative to total SHAP
summarise(weighted_shap = sum(total_shap * weight), .groups = "drop") %>%
pivot_wider(names_from = category, values_from = weighted_shap, values_fill = 0) %>%
# Compute confidence intervals for weighted SHAP values
pivot_longer(-c(trait, bootstrap_id), names_to = "category", values_to = "shap_value") %>%
group_by(trait, category) %>%
summarise(
mean_shap = mean(shap_value),
sd_shap = sd(shap_value),
lower_ci = quantile(shap_value, probs = 0.025),
upper_ci = quantile(shap_value, probs = 0.975),
.groups = "drop")
shap_summary_boot %>%
ggplot(aes(x = mean_shap, y = trait, color = category)) +
geom_point(size = 3) +
geom_errorbar(aes(y = trait, x = mean_shap, xmin = lower_ci, xmax = upper_ci, color = category), width = 0.3) +
scale_y_discrete(labels = c(
"wood_density" = "Wood Density",
"bark_thickness" = "Bark Thickness",
"conduit_diam" = "Conduit Diameter",
"leaf_n" = "Leaf Nitrogen",
"specific_leaf_area" = "Specific Leaf Area",
"seed_dry_mass" = "Seed Dry Mass",
"shade_tolerance" = "Shade Tolerance",
"height" = "Tree Height")) +
scale_color_manual(values = c("Temporal Succession" = "black", "Environmental Filtering" = "darkred")) +
theme_clean() +
labs( x = "Total SHAP Importance (Weighted Normalization)",
y = NULL,
color = "Predictor",
caption = "Points represent bootstrapped median values. Error bars indicate 95% confidence intervals from bootstrapping.") +
theme(text = element_text(size = 14), legend.position = "top")
##################################################################################
############################ OVP analysis: summer day ############################
##################################################################################
rm(list = ls()); gc() # make sure environment is clean
export = TRUE # Export?
# Activate package libraries
library(patchwork)
library(feather)
library(mgcv)
library(car)
library(ggeffects)
library(MASS)
library(emmeans)
library(gamm4)
library(effects)
library(stats)
library(lme4)
library(lmerTest)
library(performance)
library(nlme)
library(piecewiseSEM)
library(tidyverse)
# Set WD and get source code functions
setwd("/Users/serpent/Documents/VHL/OVP/Code/Analysis")
source(url("https://raw.githubusercontent.com/MerlinWe/ovp_thermo/main/Analysis/SEM_(MWE)/functions.R"))
# Read data
dat <- read_csv("/Users/serpent/Documents/VHL/OVP/Data/ovp_data_10_12_24.csv")
# Get summer day subset
summer <- dat %>% prep_ovp("Summer", "day") # ok
# Create quadratic terms explicitly in data
summer <- summer %>%
mutate(
phase_mean_CT_sq = phase_mean_CT^2,
day_season_sq = day_season^2,
weight_sq = weight^2,
day_season = as.numeric(day_season))
top_HR <- nlme::lme(
fixed = mean_heartrate ~ season_year + phase_mean_CT + phase_mean_CT_sq +
day_season + day_season_sq + weight + mean_activity_percent,
data = summer,
random = ~1 + mean_activity_percent | ID_phase,
correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_BT <- nlme::lme(
fixed = mean_BT_smooth ~ season_year + phase_mean_CT +
day_season + day_season_sq + mean_activity_percent,
data = summer,
random = ~1 | ID_phase,
correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_ACT <- nlme::lme(
fixed = mean_activity_percent ~ season_year + phase_mean_CT +
weight + weight_sq,
data = summer,
random = ~1 + phase_mean_CT | ID_phase,
correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_HR <- nlme::lme(
fixed = mean_heartrate ~ season_year + phase_mean_CT + phase_mean_CT_sq +
day_season + day_season_sq + weight + mean_activity_percent,
data = summer,
random = ~1 + mean_activity_percent | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_BT <- nlme::lme(
fixed = mean_BT_smooth ~ season_year + phase_mean_CT +
day_season + day_season_sq + mean_activity_percent,
data = summer,
random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_ACT <- nlme::lme(
fixed = mean_activity_percent ~ season_year + phase_mean_CT +
weight + weight_sq,
data = summer,
random = ~1 + phase_mean_CT | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
# Create the piecewise SEM
psem_model <- piecewiseSEM::psem(
top_HR,
top_BT,
top_ACT)
summary(psem_model)
top_HR <- nlme::lme(
fixed = mean_heartrate ~ season_year + phase_mean_CT + phase_mean_CT_sq +
day_season + day_season_sq + weight + mean_activity_percent,
data = summer,
#random = ~1 + mean_activity_percent | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_BT <- nlme::lme(
fixed = mean_BT_smooth ~ season_year + phase_mean_CT +
day_season + day_season_sq + mean_activity_percent,
data = summer,
#random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_ACT <- nlme::lme(
fixed = mean_activity_percent ~ season_year + phase_mean_CT +
weight + weight_sq,
data = summer,
#random = ~1 + phase_mean_CT | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_HR <- nlme::lme(
fixed = mean_heartrate ~ season_year + phase_mean_CT + phase_mean_CT_sq +
day_season + day_season_sq + weight + mean_activity_percent,
data = summer,
random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_BT <- nlme::lme(
fixed = mean_BT_smooth ~ season_year + phase_mean_CT +
day_season + day_season_sq + mean_activity_percent,
data = summer,
random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_ACT <- nlme::lme(
fixed = mean_activity_percent ~ season_year + phase_mean_CT +
weight + weight_sq,
data = summer,
random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
# Create the piecewise SEM
psem_model <- piecewiseSEM::psem(
top_HR,
top_BT,
top_ACT)
summary(psem_model)
top_HR <- nlme::lme(
fixed = mean_heartrate ~ season_year + phase_mean_CT + phase_mean_CT_sq +
day_season + day_season_sq + weight + mean_activity_percent,
data = summer,
random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
#method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_BT <- nlme::lme(
fixed = mean_BT_smooth ~ season_year + phase_mean_CT +
day_season + day_season_sq + mean_activity_percent,
data = summer,
random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
#method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_ACT <- nlme::lme(
fixed = mean_activity_percent ~ season_year + phase_mean_CT +
weight + weight_sq,
data = summer,
random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
#method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
# Create the piecewise SEM
psem_model <- piecewiseSEM::psem(
top_HR,
top_BT,
top_ACT)
summary(psem_model)
top_BT <- nlme::lme(
fixed = mean_BT_smooth ~ season_year + phase_mean_CT +
day_season + day_season_sq + mean_activity_percent,
data = summer,
random = ~1,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
#method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
top_HR <- glm(
fixed = mean_heartrate ~ season_year + phase_mean_CT + phase_mean_CT_sq +
day_season + day_season_sq + weight + mean_activity_percent,
data = summer,
#random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
#method = "ML",
na.action = na.exclude,
control = lmeControl(opt = "optim"))
#control = lmeControl(opt = "optim"))
top_HR <- glm(
top_HR <- glm(
fixed = mean_heartrate ~ season_year + phase_mean_CT + phase_mean_CT_sq +
day_season + day_season_sq + weight + mean_activity_percent,
data = summer,
#random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
#method = "ML",
na.action = na.exclude,
#control = lmeControl(opt = "optim")
)
top_HR <- glm(
top_BT <- glm(
fixed = mean_BT_smooth ~ season_year + phase_mean_CT +
day_season + day_season_sq + mean_activity_percent,
data = summer,
#random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
#method = "ML",
na.action = na.exclude,
#control = lmeControl(opt = "optim")
)
top_HR <- glm(
mean_heartrate ~ season_year + phase_mean_CT + phase_mean_CT_sq +
day_season + day_season_sq + weight + mean_activity_percent,
data = summer,
#random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
#method = "ML",
na.action = na.exclude,
#control = lmeControl(opt = "optim")
)
top_BT <- glm(
mean_BT_smooth ~ season_year + phase_mean_CT +
day_season + day_season_sq + mean_activity_percent,
data = summer,
#random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
#method = "ML",
na.action = na.exclude,
#control = lmeControl(opt = "optim")
)
top_ACT <- glm(
mean_activity_percent ~ season_year + phase_mean_CT +
weight + weight_sq,
data = summer,
random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
#method = "ML",
na.action = na.exclude,
#control = lmeControl(opt = "optim")
)
top_ACT <- glm(
mean_activity_percent ~ season_year + phase_mean_CT +
weight + weight_sq,
data = summer,
#random = ~1 | ID_phase,
#correlation = corGaus(form = ~ as.numeric(day_season) | ID_phase, nugget = TRUE),
#method = "ML",
na.action = na.exclude,
#control = lmeControl(opt = "optim")
)
# Create the piecewise SEM
psem_model <- piecewiseSEM::psem(
top_HR,
top_BT,
top_ACT)
summary(psem_model)
